{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8e24b8e-4403-4f1c-93c2-244b16e89324",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Gold Layer\n",
    "- final transformation\n",
    "- like dim & fact table conversion\n",
    "- aggregation related things\n",
    "\n",
    "but now, will do some simple transformation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c0e8d0f-0143-451c-9afe-3c619ee37c87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Doing transformation for all tables (changing column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85578a6d-71c6-4737-aff7-09ea61c30a7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name=[]\n",
    "\n",
    "for i in dbutils.fs.ls(\"mnt/silver/SalesLT/\"):\n",
    "    table_name.append(i.name.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ab6789-d727-42b0-9b71-d6c7a5a5e095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "for name in table_name:\n",
    "    path='/mnt/silver/SalesLT/'+name\n",
    "    # print(path)\n",
    "    df=spark.read.format('delta').load(path)\n",
    "\n",
    "    # Get the list of column names\n",
    "    column_names=df.columns\n",
    "\n",
    "    for old_col_name in column_names:\n",
    "        # convert column name from ColumnName to column_name format\n",
    "        new_col_name=\"\".join([\"_\" + char if char.isupper() and not old_col_name[i - 1].isupper() else char for i, char in enumerate(old_col_name)]).lstrip(\"_\") \n",
    "\n",
    "        # Change the column name using withColumnRenamed and regexp_replace\n",
    "        df=df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "    output_path='/mnt/gold/SalesLT/' +name +'/'\n",
    "    df.write.format('delta').mode(\"overwrite\").save(output_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver to gold",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
