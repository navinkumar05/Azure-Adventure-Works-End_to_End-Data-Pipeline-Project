---
Created: 2024-04-23
tags:
  - project
  - Azure
  - Data_ingestion
aliases:
---
# [[01. Data Ingestion using ADF]]


Lunch Data Factory

## Integration Runtime

 This is used to make connection between Local SQL server and Azure

### Create Integration runtime for on perm database
1. manage > integration runtime
![[Pasted image 20240423201304.png]]

2. click new, select Azure, Self hosted>continue
3. select self-hosted
![[Pasted image 20240423201654.png]]

4. choose the name & description
![[Pasted image 20240423201907.png]]

5. select `Option 1: Express setup`
![[Pasted image 20240423202047.png]]

6. setup > close
![[Pasted image 20240423202346.png | 600]]

7. once complete the setup, you can able to see `SelfHostedIntegrationRuntion` 
![[Pasted image 20240423203829.png]]

8. completed the setup.
![[Pasted image 20240423204617.png]]


## Linked service

![[Pasted image 20240424070239.png]]

> **Source linked service:** This would contain the connection information (such as server name, database name, authentication method, etc.) for the Azure SQL Database.

### key vault Linked service

![[Pasted image 20240423214947.png | 600]]

It can able to access, doesn't have permission
![[Pasted image 20240423215054.png]]

[watch this to give permission to ADF](https://youtu.be/HCQWrbi6QfY?si=QlCa7tvRZABtWhD_&t=2265)

### SQL Server Linked service

![[Pasted image 20240424070556.png]]


### Storage Account linked service

![[Pasted image 20240424071006.png]]
## Data Ingestion

### Create pipeline
1. Author > Pipeline > `+` pipeline 
2. Under properties > choose name 
### copy Address table
![[Pasted image 20240423213604.png]]
#### Source

1. select ==SQL server== as source
![[Pasted image 20240423213850.png]]

2. Dataset name
![[Pasted image 20240423214057.png]]

3. linked service > choose already we created [[01. Data Ingestion using ADF#SQL Server Linked service | SQL Server Linked service]]

#### Sink
1. `+` >  choose data set --> Select Azure data lake storage gen 2
2. select format > ==parquet==
3. set properties 
	1. Name
	2. linked service > choose storage account linked service. [[01. Data Ingestion using ADF#Storage Account linked service | Storage Account linked service]]
4. location > bronze
### Run the job

1. validate the pipeline

2. click Debug (for quick runs)
![[Pasted image 20240424071533.png]]

Result
![[Pasted image 20240424072638.png]]

## Copy all tables

![[Pasted image 20240424103618.png]]

### lookup 
![[Pasted image 20240424092051.png]]

#### Settings
**source data set**
1. Select `sql server`
2. set properties. don't select all tables
![[Pasted image 20240424092435.png]]

Use query
```sql
-- select all tables from schema 'SalesLT'
SELECT t.name AS TableName,
       s.name AS SchemaName
FROM sys.tables t
INNER JOIN sys.schemas s ON t.schema_id=s.schema_id
WHERE s.name='SalesLT';
```

![[Pasted image 20240424093351.png]]

paste the query and see the preview data
![[Pasted image 20240424093530.png]]
#### Debug the lookup

Two things you can note in ==OUTPUT==
- input (query)
- output
![[Pasted image 20240424094215.png]]
##### output

```
{
	"count": 10,
	"value": [
		{
			"TableName": "Address",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "Customer",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "CustomerAddress",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "Product",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "ProductCategory",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "ProductDescription",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "ProductModel",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "ProductModelProductDescription",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "SalesOrderDetail",
			"SchemaName": "SalesLT"
		},
		{
			"TableName": "SalesOrderHeader",
			"SchemaName": "SalesLT"
		}
	],
	"effectiveIntegrationRuntime": "integrationRuntime1",
	"billingReference": {
		"activityType": "PipelineActivity",
		"billableDuration": [
			{
				"meterType": "SelfhostedIR",
				"duration": 0.016666666666666666,
				"unit": "Hours"
			}
		],
		"totalBillableDuration": [
			{
				"meterType": "AzureIR",
				"duration": 0.06666666666666667,
				"unit": "Hours"
			}
		]
	},
	"durationInQueue": {
		"integrationRuntimeQueue": 7
	}
}
```

### ForEach

need value form lookup
![[Pasted image 20240424095000.png]]

#### copy data

![[Pasted image 20240424100256.png]]


### Sink


![[Pasted image 20240424073754.png]]

1. Create parameters
![[Pasted image 20240424102159.png]]

2 parameters (schema name, table name)
![[Pasted image 20240424102338.png]]

2. now you can see here
![[Pasted image 20240424102439.png]]

```
-- folder name
@concat(dataset().SchemaName, '/', dataset().TableName)

-- file name
@concat(dataset().TableName, '.parquet ')
```

![[Pasted image 20240424102727.png]]

3. Publish the changes
4. validate and run
![[Pasted image 20240424103202.png]]

5. Result
![[Pasted image 20240424103435.png]]

![[Pasted image 20240424103454.png]]
### monitor the pipeline status

![[Pasted image 20240424103313.png]]