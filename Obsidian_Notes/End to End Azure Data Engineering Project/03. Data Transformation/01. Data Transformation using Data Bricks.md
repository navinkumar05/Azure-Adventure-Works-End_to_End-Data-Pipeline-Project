---
Created: 2024-04-24
tags:
  - project
  - Azure
  - DataBricks
aliases:
---
# [[01. Data Transformation using Data Bricks]]

## Compute
> Every process need CPU power

used Azure Premium data bricks
### Create compute

![[Pasted image 20240424120314.png]]

### Create Notebook
![[Pasted image 20240424120656.png]]

### Mount data lake storage on DataBricks

```python
configs = {
  "fs.azure.account.auth.type": "CustomAccessToken",
  "fs.azure.account.custom.token.provider.class": spark.conf.get("spark.databricks.passthrough.adls.gen2.tokenProviderClassName")
}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = "abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/",
  mount_point = "/mnt/<mount-name>",
  extra_configs = configs)
```

list files
```python
dbutils.fs.ls("/mnt/bronze/SalesLT")
```


## Reference

[Access Azure Data Lake Storage using Microsoft Entra ID (formerly Azure Active Directory) credential passthrough (legacy) - Azure Databricks | Microsoft Learn](https://learn.microsoft.com/en-us/azure/databricks/archive/credential-passthrough/adls-passthrough#--azure-data-lake-storage-gen2-1)

[Mounting cloud object storage on Azure Databricks - Azure Databricks | Microsoft Learn](https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts)